{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "sys.path.append(r\"D:\\FinRL-master\\FinRL-master\")\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
        "# it has the columns and index in the form that could be make into the environment. \n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_ddpg = False\n",
        "if_using_ppo = True\n",
        "if_using_td3 = False\n",
        "if_using_sac = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 4 algorithms (DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "M2YadjfnLwgt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001, 'device': 'cuda'}\n",
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tCDa78rqfO_a"
      },
      "outputs": [],
      "source": [
        "trained_ddpg = model_ddpg.learn(total_timesteps=50000, tb_log_name='ddpg') if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg2\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "y5D5PFUhMzSV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128, 'device': 'cuda'}\n",
            "Using cuda device\n",
            "Logging to results/ppo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jvija\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 65         |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 31         |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | 1.8889455  |\n",
            "|    reward_max      | 7.9131746  |\n",
            "|    reward_mean     | 0.10799384 |\n",
            "|    reward_min      | -14.483849 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 87          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 46          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014609091 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0094     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.77        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0226     |\n",
            "|    reward               | 1.7837751   |\n",
            "|    reward_max           | 27.060715   |\n",
            "|    reward_mean          | 0.072763674 |\n",
            "|    reward_min           | -39.430897  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 14.9        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 98        |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 62        |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0198392 |\n",
            "|    clip_fraction        | 0.216     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -41.2     |\n",
            "|    explained_variance   | -0.0126   |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 68.3      |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -0.0187   |\n",
            "|    reward               | -0.639987 |\n",
            "|    reward_max           | 42.32973  |\n",
            "|    reward_mean          | 0.1382559 |\n",
            "|    reward_min           | -33.59146 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 125       |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 104         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012448589 |\n",
            "|    clip_fraction        | 0.094       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0211     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.7        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    reward               | -2.325756   |\n",
            "|    reward_max           | 15.463324   |\n",
            "|    reward_mean          | 0.017471926 |\n",
            "|    reward_min           | -23.631163  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 68          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 109        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 93         |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01612953 |\n",
            "|    clip_fraction        | 0.161      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.3      |\n",
            "|    explained_variance   | -0.00707   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 19         |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0163    |\n",
            "|    reward               | 2.4006758  |\n",
            "|    reward_max           | 19.332535  |\n",
            "|    reward_mean          | 0.04833753 |\n",
            "|    reward_min           | -25.468197 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 58.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 112         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 109         |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016908789 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00699    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.1        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0192     |\n",
            "|    reward               | 1.0265555   |\n",
            "|    reward_max           | 10.474302   |\n",
            "|    reward_mean          | 0.09033908  |\n",
            "|    reward_min           | -10.37046   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 43.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 114         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 124         |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021618098 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.0366     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.8        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    reward               | -0.20146017 |\n",
            "|    reward_max           | 30.741842   |\n",
            "|    reward_mean          | 0.10597076  |\n",
            "|    reward_min           | -30.603624  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 21.4        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 115        |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 141        |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01803964 |\n",
            "|    clip_fraction        | 0.198      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.4      |\n",
            "|    explained_variance   | -0.0014    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 41.1       |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | -0.016     |\n",
            "|    reward               | 1.4416882  |\n",
            "|    reward_max           | 12.649965  |\n",
            "|    reward_mean          | 0.12219913 |\n",
            "|    reward_min           | -14.008602 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 91.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 117         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 157         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020623393 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.00929     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.2        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    reward               | 0.43600875  |\n",
            "|    reward_max           | 28.243385   |\n",
            "|    reward_mean          | 0.1123852   |\n",
            "|    reward_min           | -27.664286  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 26.5        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 117        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 174        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01626267 |\n",
            "|    clip_fraction        | 0.167      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.5      |\n",
            "|    explained_variance   | 0.00675    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 38.1       |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.0175    |\n",
            "|    reward               | -3.1414185 |\n",
            "|    reward_max           | 24.557148  |\n",
            "|    reward_mean          | 0.11045558 |\n",
            "|    reward_min           | -24.604761 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 78.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 118         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 190         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015386403 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.00748     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.2        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0164     |\n",
            "|    reward               | 0.72302616  |\n",
            "|    reward_max           | 13.078015   |\n",
            "|    reward_mean          | 0.07983746  |\n",
            "|    reward_min           | -16.629772  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 55.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 205         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021625157 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.0138      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.4        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0133     |\n",
            "|    reward               | -0.21904953 |\n",
            "|    reward_max           | 27.37144    |\n",
            "|    reward_mean          | 0.13458839  |\n",
            "|    reward_min           | -34.109745  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 25          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 220         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026424168 |\n",
            "|    clip_fraction        | 0.281       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.0061      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 53.5        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0143     |\n",
            "|    reward               | -0.7904646  |\n",
            "|    reward_max           | 13.772194   |\n",
            "|    reward_mean          | 0.09042725  |\n",
            "|    reward_min           | -10.623722  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 93.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 121         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 235         |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.01942156  |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.107      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.89        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0148     |\n",
            "|    reward               | 0.6546227   |\n",
            "|    reward_max           | 23.999838   |\n",
            "|    reward_mean          | 0.033592135 |\n",
            "|    reward_min           | -25.839293  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 19.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 122         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 250         |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024138125 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | 0.0111      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.7        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    reward               | -4.1261983  |\n",
            "|    reward_max           | 19.241243   |\n",
            "|    reward_mean          | 0.08868941  |\n",
            "|    reward_min           | -22.393162  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 74.2        |\n",
            "-----------------------------------------\n",
            "day: 3458, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3709548.69\n",
            "total_reward: 2709548.69\n",
            "total_cost: 438537.70\n",
            "total_trades: 92496\n",
            "Sharpe: 0.682\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 265         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020638    |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.023      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.6        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0139     |\n",
            "|    reward               | -0.25808316 |\n",
            "|    reward_max           | 11.784981   |\n",
            "|    reward_mean          | 0.05902031  |\n",
            "|    reward_min           | -11.433353  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 37.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 281         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023972511 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.011      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.6        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0169     |\n",
            "|    reward               | 1.1076499   |\n",
            "|    reward_max           | 25.757765   |\n",
            "|    reward_mean          | 0.055603795 |\n",
            "|    reward_min           | -27.227833  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 29.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 297         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022791829 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.0101     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.1        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | -0.8291528  |\n",
            "|    reward_max           | 8.713168    |\n",
            "|    reward_mean          | 0.06542063  |\n",
            "|    reward_min           | -10.383534  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 63          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 313         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.0358982   |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.119      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.53        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.011      |\n",
            "|    reward               | 0.009152416 |\n",
            "|    reward_max           | 19.964027   |\n",
            "|    reward_mean          | 0.029857306 |\n",
            "|    reward_min           | -21.272043  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 330         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02136327  |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.00782     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.3        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0123     |\n",
            "|    reward               | -0.9056099  |\n",
            "|    reward_max           | 21.813847   |\n",
            "|    reward_mean          | 0.091521166 |\n",
            "|    reward_min           | -27.49052   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 32          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 346         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023272095 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.0413     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.41        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0153     |\n",
            "|    reward               | -2.5905926  |\n",
            "|    reward_max           | 12.47646    |\n",
            "|    reward_mean          | 0.020996924 |\n",
            "|    reward_min           | -16.279182  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 27.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 22         |\n",
            "|    time_elapsed         | 361        |\n",
            "|    total_timesteps      | 45056      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02150208 |\n",
            "|    clip_fraction        | 0.271      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42        |\n",
            "|    explained_variance   | 0.0192     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 8.26       |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | -0.0154    |\n",
            "|    reward               | 0.69992507 |\n",
            "|    reward_max           | 22.714104  |\n",
            "|    reward_mean          | 0.04376625 |\n",
            "|    reward_min           | -27.819746 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 26.2       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 23         |\n",
            "|    time_elapsed         | 377        |\n",
            "|    total_timesteps      | 47104      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02410469 |\n",
            "|    clip_fraction        | 0.237      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42        |\n",
            "|    explained_variance   | -0.00632   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 20.7       |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.00723   |\n",
            "|    reward               | 0.75952816 |\n",
            "|    reward_max           | 7.7519097  |\n",
            "|    reward_mean          | 0.06494189 |\n",
            "|    reward_min           | -10.982784 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 56.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 393         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.045711085 |\n",
            "|    clip_fraction        | 0.31        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | -0.00545    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.33        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | -0.14667873 |\n",
            "|    reward_max           | 19.195404   |\n",
            "|    reward_mean          | 0.019760294 |\n",
            "|    reward_min           | -21.244572  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 12.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 409         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022681175 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | -0.0104     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.6        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00806    |\n",
            "|    reward               | -1.0395906  |\n",
            "|    reward_max           | 18.259789   |\n",
            "|    reward_mean          | 0.096228264 |\n",
            "|    reward_min           | -22.338758  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 30.1        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 125        |\n",
            "|    iterations           | 26         |\n",
            "|    time_elapsed         | 424        |\n",
            "|    total_timesteps      | 53248      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03321216 |\n",
            "|    clip_fraction        | 0.334      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.3      |\n",
            "|    explained_variance   | 0.00977    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 9.68       |\n",
            "|    n_updates            | 250        |\n",
            "|    policy_gradient_loss | -0.00487   |\n",
            "|    reward               | -1.9999722 |\n",
            "|    reward_max           | 8.756836   |\n",
            "|    reward_mean          | 0.05048074 |\n",
            "|    reward_min           | -12.376111 |\n",
            "|    std                  | 1.04       |\n",
            "|    value_loss           | 23.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 439         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029856991 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.0299      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.1        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    reward               | 0.008289699 |\n",
            "|    reward_max           | 23.36823    |\n",
            "|    reward_mean          | 0.1531596   |\n",
            "|    reward_min           | -28.378323  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 21.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 454         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026782023 |\n",
            "|    clip_fraction        | 0.292       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.00695     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 49.4        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.00321    |\n",
            "|    reward               | 0.05927322  |\n",
            "|    reward_max           | 8.695732    |\n",
            "|    reward_mean          | 0.063309126 |\n",
            "|    reward_min           | -7.907365   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 83.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 469         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023305686 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | -0.00598    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.02        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    reward               | -1.6674875  |\n",
            "|    reward_max           | 19.456787   |\n",
            "|    reward_mean          | 0.1047648   |\n",
            "|    reward_min           | -21.849733  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 13.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 485         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026312858 |\n",
            "|    clip_fraction        | 0.305       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.000798    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 37.2        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.00752    |\n",
            "|    reward               | 2.525016    |\n",
            "|    reward_max           | 22.170403   |\n",
            "|    reward_mean          | 0.085075304 |\n",
            "|    reward_min           | -29.062145  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 96.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 502         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016804267 |\n",
            "|    clip_fraction        | 0.156       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.0398      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.8        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    reward               | 0.33072507  |\n",
            "|    reward_max           | 12.991386   |\n",
            "|    reward_mean          | 0.07865771  |\n",
            "|    reward_min           | -19.107836  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 44.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 517         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024211619 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.0388      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.8        |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.00869    |\n",
            "|    reward               | -1.4616755  |\n",
            "|    reward_max           | 22.980755   |\n",
            "|    reward_mean          | 0.091448575 |\n",
            "|    reward_min           | -26.951826  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 51.6        |\n",
            "-----------------------------------------\n",
            "day: 3458, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4151611.58\n",
            "total_reward: 3151611.58\n",
            "total_cost: 440705.75\n",
            "total_trades: 91394\n",
            "Sharpe: 0.664\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 534         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017207213 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | -0.0104     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 42.7        |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.00787    |\n",
            "|    reward               | 0.26067403  |\n",
            "|    reward_max           | 18.553219   |\n",
            "|    reward_mean          | 0.064853966 |\n",
            "|    reward_min           | -10.719061  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 89.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 550         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042013407 |\n",
            "|    clip_fraction        | 0.384       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | -0.00299    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.84        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.00798    |\n",
            "|    reward               | 1.6942493   |\n",
            "|    reward_max           | 21.950438   |\n",
            "|    reward_mean          | 0.11121181  |\n",
            "|    reward_min           | -26.084244  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 21.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 565         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033657692 |\n",
            "|    clip_fraction        | 0.265       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0149      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 66.3        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.00777    |\n",
            "|    reward               | -0.2971976  |\n",
            "|    reward_max           | 10.612489   |\n",
            "|    reward_mean          | 0.08952583  |\n",
            "|    reward_min           | -12.59705   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 96.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 580         |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020568714 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0404      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.26        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    reward               | -0.3921702  |\n",
            "|    reward_max           | 19.681204   |\n",
            "|    reward_mean          | 0.08760294  |\n",
            "|    reward_min           | -22.546484  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 17.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 596         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027095169 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0259      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 23.3        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    reward               | 8.904923    |\n",
            "|    reward_max           | 28.304794   |\n",
            "|    reward_mean          | 0.11661053  |\n",
            "|    reward_min           | -30.6085    |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 43.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 612         |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022081438 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | -0.0129     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 36.3        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00741    |\n",
            "|    reward               | 6.1351347   |\n",
            "|    reward_max           | 14.138284   |\n",
            "|    reward_mean          | 0.053306468 |\n",
            "|    reward_min           | -14.422177  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 64.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 628         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022240076 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | -0.0033     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.3        |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.016      |\n",
            "|    reward               | 0.04081026  |\n",
            "|    reward_max           | 19.23656    |\n",
            "|    reward_mean          | 0.068040736 |\n",
            "|    reward_min           | -20.583181  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 27.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 644         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030523755 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.00182     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.6        |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.00905    |\n",
            "|    reward               | 0.6141314   |\n",
            "|    reward_max           | 10.341098   |\n",
            "|    reward_mean          | 0.09358328  |\n",
            "|    reward_min           | -13.399427  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 52.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 660         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031591825 |\n",
            "|    clip_fraction        | 0.323       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | -0.037      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.67        |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    reward               | -1.080053   |\n",
            "|    reward_max           | 23.263752   |\n",
            "|    reward_mean          | 0.07980955  |\n",
            "|    reward_min           | -24.264328  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 17          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 676         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021542478 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.00615     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16.2        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00994    |\n",
            "|    reward               | 0.007926289 |\n",
            "|    reward_max           | 19.82035    |\n",
            "|    reward_mean          | 0.07967629  |\n",
            "|    reward_min           | -19.048916  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 48.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 692         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013612414 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | -0.0125     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.3        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.00911    |\n",
            "|    reward               | 1.0752362   |\n",
            "|    reward_max           | 14.061004   |\n",
            "|    reward_mean          | 0.04775533  |\n",
            "|    reward_min           | -10.518412  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 33.2        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 127        |\n",
            "|    iterations           | 44         |\n",
            "|    time_elapsed         | 708        |\n",
            "|    total_timesteps      | 90112      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02482895 |\n",
            "|    clip_fraction        | 0.257      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43        |\n",
            "|    explained_variance   | 0.0095     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 10.3       |\n",
            "|    n_updates            | 430        |\n",
            "|    policy_gradient_loss | -0.0064    |\n",
            "|    reward               | 0.02338801 |\n",
            "|    reward_max           | 22.915615  |\n",
            "|    reward_mean          | 0.11926337 |\n",
            "|    reward_min           | -24.420742 |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 27.8       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 127        |\n",
            "|    iterations           | 45         |\n",
            "|    time_elapsed         | 725        |\n",
            "|    total_timesteps      | 92160      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0265658  |\n",
            "|    clip_fraction        | 0.29       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.1      |\n",
            "|    explained_variance   | 0.00719    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 52.5       |\n",
            "|    n_updates            | 440        |\n",
            "|    policy_gradient_loss | -0.00727   |\n",
            "|    reward               | 2.5124538  |\n",
            "|    reward_max           | 7.01272    |\n",
            "|    reward_mean          | 0.08456293 |\n",
            "|    reward_min           | -13.050663 |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 105        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 741         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034054253 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.0555      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.93        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00885    |\n",
            "|    reward               | 1.660676    |\n",
            "|    reward_max           | 21.162767   |\n",
            "|    reward_mean          | 0.08205186  |\n",
            "|    reward_min           | -20.36853   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 13.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 758         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02215055  |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.019       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.8        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.00479    |\n",
            "|    reward               | -0.76466775 |\n",
            "|    reward_max           | 21.865166   |\n",
            "|    reward_mean          | 0.11704497  |\n",
            "|    reward_min           | -21.159369  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 54          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 774         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03196343  |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | -0.0277     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.3         |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.00797    |\n",
            "|    reward               | -0.12763117 |\n",
            "|    reward_max           | 18.651785   |\n",
            "|    reward_mean          | 0.03762165  |\n",
            "|    reward_min           | -13.00998   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 36.7        |\n",
            "-----------------------------------------\n",
            "day: 3458, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3716795.46\n",
            "total_reward: 2716795.46\n",
            "total_cost: 437548.37\n",
            "total_trades: 91351\n",
            "Sharpe: 0.679\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 126        |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 792        |\n",
            "|    total_timesteps      | 100352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03187307 |\n",
            "|    clip_fraction        | 0.244      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.3      |\n",
            "|    explained_variance   | 0.101      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 14.3       |\n",
            "|    n_updates            | 480        |\n",
            "|    policy_gradient_loss | -0.00307   |\n",
            "|    reward               | 0.18218519 |\n",
            "|    reward_max           | 29.99533   |\n",
            "|    reward_mean          | 0.08473631 |\n",
            "|    reward_min           | -32.660305 |\n",
            "|    std                  | 1.08       |\n",
            "|    value_loss           | 34.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 810         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03698369  |\n",
            "|    clip_fraction        | 0.238       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | -0.00215    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 37          |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | 0.000315    |\n",
            "|    reward               | -0.8003814  |\n",
            "|    reward_max           | 9.560489    |\n",
            "|    reward_mean          | 0.105472185 |\n",
            "|    reward_min           | -15.022968  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 66.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 825         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026589675 |\n",
            "|    clip_fraction        | 0.283       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.9         |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.0122     |\n",
            "|    reward               | 0.45796624  |\n",
            "|    reward_max           | 27.229792   |\n",
            "|    reward_mean          | 0.0534801   |\n",
            "|    reward_min           | -41.635292  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 14.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 842         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030106291 |\n",
            "|    clip_fraction        | 0.306       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.00812     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.2        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.00214    |\n",
            "|    reward               | -1.1746417  |\n",
            "|    reward_max           | 28.55524    |\n",
            "|    reward_mean          | 0.08934867  |\n",
            "|    reward_min           | -44.29341   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 71.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 858         |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032317758 |\n",
            "|    clip_fraction        | 0.328       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.0209      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.7        |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.00673    |\n",
            "|    reward               | -0.18222661 |\n",
            "|    reward_max           | 13.543763   |\n",
            "|    reward_mean          | 0.10407763  |\n",
            "|    reward_min           | -11.985678  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 52.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 875         |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035051137 |\n",
            "|    clip_fraction        | 0.325       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.0438      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.8        |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | -1.679287   |\n",
            "|    reward_max           | 25.927147   |\n",
            "|    reward_mean          | 0.091677524 |\n",
            "|    reward_min           | -24.097582  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 38.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 892         |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023851775 |\n",
            "|    clip_fraction        | 0.312       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.0162      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.8        |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00276    |\n",
            "|    reward               | 0.76619315  |\n",
            "|    reward_max           | 18.091143   |\n",
            "|    reward_mean          | 0.07420049  |\n",
            "|    reward_min           | -13.059619  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 73.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 910         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039524887 |\n",
            "|    clip_fraction        | 0.364       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.046       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.8         |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.00727    |\n",
            "|    reward               | -0.17498231 |\n",
            "|    reward_max           | 24.652401   |\n",
            "|    reward_mean          | 0.09795502  |\n",
            "|    reward_min           | -25.101238  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 15.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 125        |\n",
            "|    iterations           | 57         |\n",
            "|    time_elapsed         | 927        |\n",
            "|    total_timesteps      | 116736     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02963234 |\n",
            "|    clip_fraction        | 0.312      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.7      |\n",
            "|    explained_variance   | 0.03       |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 39.6       |\n",
            "|    n_updates            | 560        |\n",
            "|    policy_gradient_loss | -0.00682   |\n",
            "|    reward               | 7.1521316  |\n",
            "|    reward_max           | 30.216536  |\n",
            "|    reward_mean          | 0.1268565  |\n",
            "|    reward_min           | -35.98593  |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 72         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 944         |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025364948 |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.0297      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.2        |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00864    |\n",
            "|    reward               | -1.3374349  |\n",
            "|    reward_max           | 26.509628   |\n",
            "|    reward_mean          | 0.17885461  |\n",
            "|    reward_min           | -26.951769  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 60.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 961         |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026932634 |\n",
            "|    clip_fraction        | 0.254       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.02        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 61.3        |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.008      |\n",
            "|    reward               | 4.539459    |\n",
            "|    reward_max           | 36.957413   |\n",
            "|    reward_mean          | 0.13093461  |\n",
            "|    reward_min           | -40.665024  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 127         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 978         |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03810028  |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.00577     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 68.1        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | 0.00182     |\n",
            "|    reward               | -2.6731226  |\n",
            "|    reward_max           | 19.687225   |\n",
            "|    reward_mean          | 0.084991604 |\n",
            "|    reward_min           | -12.22672   |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 108         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 125        |\n",
            "|    iterations           | 61         |\n",
            "|    time_elapsed         | 995        |\n",
            "|    total_timesteps      | 124928     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03691902 |\n",
            "|    clip_fraction        | 0.304      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.9      |\n",
            "|    explained_variance   | 0.0336     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 14.4       |\n",
            "|    n_updates            | 600        |\n",
            "|    policy_gradient_loss | -0.00512   |\n",
            "|    reward               | -5.6455936 |\n",
            "|    reward_max           | 33.045723  |\n",
            "|    reward_mean          | 0.09098977 |\n",
            "|    reward_min           | -37.60813  |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 24         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1012        |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018126834 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.0615      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 42.6        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.00646    |\n",
            "|    reward               | 1.2327536   |\n",
            "|    reward_max           | 15.802881   |\n",
            "|    reward_mean          | 0.14519979  |\n",
            "|    reward_min           | -20.016727  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 90.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 1028        |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021899655 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.0615      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.73        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00457    |\n",
            "|    reward               | 0.77221245  |\n",
            "|    reward_max           | 34.918743   |\n",
            "|    reward_mean          | 0.10574933  |\n",
            "|    reward_min           | -40.632423  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 29          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 125        |\n",
            "|    iterations           | 64         |\n",
            "|    time_elapsed         | 1045       |\n",
            "|    total_timesteps      | 131072     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03515218 |\n",
            "|    clip_fraction        | 0.259      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44        |\n",
            "|    explained_variance   | 0.0105     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 36.5       |\n",
            "|    n_updates            | 630        |\n",
            "|    policy_gradient_loss | -0.00367   |\n",
            "|    reward               | -5.9333296 |\n",
            "|    reward_max           | 42.04746   |\n",
            "|    reward_mean          | 0.20137206 |\n",
            "|    reward_min           | -55.299088 |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 106        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1061        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034172557 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | -0.00568    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 61.9        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.0128     |\n",
            "|    reward               | 1.226701    |\n",
            "|    reward_max           | 17.567654   |\n",
            "|    reward_mean          | 0.056260973 |\n",
            "|    reward_min           | -19.430862  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 121         |\n",
            "-----------------------------------------\n",
            "day: 3458, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4508755.96\n",
            "total_reward: 3508755.96\n",
            "total_cost: 421488.84\n",
            "total_trades: 88711\n",
            "Sharpe: 0.607\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 1077        |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025272448 |\n",
            "|    clip_fraction        | 0.236       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.00428     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.9        |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.00868    |\n",
            "|    reward               | -0.31826982 |\n",
            "|    reward_max           | 41.980705   |\n",
            "|    reward_mean          | 0.115651876 |\n",
            "|    reward_min           | -65.7171    |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 60.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 1094        |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016365504 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.0089      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 215         |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.00568    |\n",
            "|    reward               | -7.9022675  |\n",
            "|    reward_max           | 27.48104    |\n",
            "|    reward_mean          | 0.20517437  |\n",
            "|    reward_min           | -27.73875   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 442         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1111        |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026213195 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | -0.0112     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 42.8        |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.0122     |\n",
            "|    reward               | -0.53282684 |\n",
            "|    reward_max           | 36.41806    |\n",
            "|    reward_mean          | -0.06131618 |\n",
            "|    reward_min           | -63.254215  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 108         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 1129        |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015411338 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | -0.0111     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 526         |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.00332    |\n",
            "|    reward               | 0.8283421   |\n",
            "|    reward_max           | 34.84044    |\n",
            "|    reward_mean          | 0.12755181  |\n",
            "|    reward_min           | -61.10254   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 410         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 1145        |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012505108 |\n",
            "|    clip_fraction        | 0.0994      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.00541     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 205         |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.00961    |\n",
            "|    reward               | 1.5514611   |\n",
            "|    reward_max           | 27.434303   |\n",
            "|    reward_mean          | 0.036937665 |\n",
            "|    reward_min           | -21.825468  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 349         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 1162        |\n",
            "|    total_timesteps      | 145408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015355714 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.0344      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 36.3        |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | -0.0124     |\n",
            "|    reward               | 0.27737132  |\n",
            "|    reward_max           | 31.857141   |\n",
            "|    reward_mean          | 0.13269444  |\n",
            "|    reward_min           | -34.978195  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 103         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 1178        |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049260605 |\n",
            "|    clip_fraction        | 0.38        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.00227     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 45.8        |\n",
            "|    n_updates            | 710         |\n",
            "|    policy_gradient_loss | 0.00714     |\n",
            "|    reward               | -0.40617192 |\n",
            "|    reward_max           | 11.322771   |\n",
            "|    reward_mean          | 0.13803622  |\n",
            "|    reward_min           | -17.336668  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 108         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 1194        |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018824922 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.205       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.4         |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    reward               | -0.27747765 |\n",
            "|    reward_max           | 36.4092     |\n",
            "|    reward_mean          | 0.05825998  |\n",
            "|    reward_min           | -41.013023  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 17          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 1210        |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022063546 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.035       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 40.8        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | 0.00218     |\n",
            "|    reward               | 4.8451996   |\n",
            "|    reward_max           | 38.636948   |\n",
            "|    reward_mean          | 0.19948402  |\n",
            "|    reward_min           | -43.079952  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 99.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 1227        |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021176247 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | -0.03       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 40.3        |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00478    |\n",
            "|    reward               | -0.6241791  |\n",
            "|    reward_max           | 18.949856   |\n",
            "|    reward_mean          | 0.06831943  |\n",
            "|    reward_min           | -21.908443  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 86.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 1243        |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022718444 |\n",
            "|    clip_fraction        | 0.281       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.0456      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19          |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.00774    |\n",
            "|    reward               | -5.193626   |\n",
            "|    reward_max           | 41.30616    |\n",
            "|    reward_mean          | 0.19268544  |\n",
            "|    reward_min           | -50.122234  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 58.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 1260        |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029411506 |\n",
            "|    clip_fraction        | 0.291       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.0278      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 69.2        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | 0.000227    |\n",
            "|    reward               | 6.999371    |\n",
            "|    reward_max           | 8.736831    |\n",
            "|    reward_mean          | 0.12531087  |\n",
            "|    reward_min           | -19.05659   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 221         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 78          |\n",
            "|    time_elapsed         | 1277        |\n",
            "|    total_timesteps      | 159744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017278459 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.113       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.68        |\n",
            "|    n_updates            | 770         |\n",
            "|    policy_gradient_loss | -0.0116     |\n",
            "|    reward               | 0.87389404  |\n",
            "|    reward_max           | 35.244328   |\n",
            "|    reward_mean          | 0.08861717  |\n",
            "|    reward_min           | -44.16264   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 19.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 1293        |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035977546 |\n",
            "|    clip_fraction        | 0.306       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.0441      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 62.9        |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | 0.00929     |\n",
            "|    reward               | 5.5484257   |\n",
            "|    reward_max           | 42.155857   |\n",
            "|    reward_mean          | 0.19168751  |\n",
            "|    reward_min           | -52.423615  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 147         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1309        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020453745 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | -0.00261    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 34.8        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.00729    |\n",
            "|    reward               | -2.6449354  |\n",
            "|    reward_max           | 24.741407   |\n",
            "|    reward_mean          | 0.09215907  |\n",
            "|    reward_min           | -26.247723  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 133         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 1325        |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021211348 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.0679      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 47.3        |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.00453    |\n",
            "|    reward               | -6.663365   |\n",
            "|    reward_max           | 38.336147   |\n",
            "|    reward_mean          | 0.11508475  |\n",
            "|    reward_min           | -50.469147  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 101         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 1341        |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.01740292  |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.0935      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 114         |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | -0.00361    |\n",
            "|    reward               | 0.7541823   |\n",
            "|    reward_max           | 17.31637    |\n",
            "|    reward_mean          | 0.108458646 |\n",
            "|    reward_min           | -12.355088  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 185         |\n",
            "-----------------------------------------\n",
            "day: 3458, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4358264.90\n",
            "total_reward: 3358264.90\n",
            "total_cost: 310623.27\n",
            "total_trades: 83404\n",
            "Sharpe: 0.722\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 1357        |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020031083 |\n",
            "|    clip_fraction        | 0.183       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.0389      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.64        |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | -0.01       |\n",
            "|    reward               | 0.90632385  |\n",
            "|    reward_max           | 30.986328   |\n",
            "|    reward_mean          | 0.07081731  |\n",
            "|    reward_min           | -36.20563   |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 26.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 125        |\n",
            "|    iterations           | 84         |\n",
            "|    time_elapsed         | 1374       |\n",
            "|    total_timesteps      | 172032     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0212521  |\n",
            "|    clip_fraction        | 0.257      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.0974     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 45         |\n",
            "|    n_updates            | 830        |\n",
            "|    policy_gradient_loss | -0.00212   |\n",
            "|    reward               | -6.2380853 |\n",
            "|    reward_max           | 18.922245  |\n",
            "|    reward_mean          | 0.18701467 |\n",
            "|    reward_min           | -23.69107  |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 107        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 125        |\n",
            "|    iterations           | 85         |\n",
            "|    time_elapsed         | 1390       |\n",
            "|    total_timesteps      | 174080     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03524316 |\n",
            "|    clip_fraction        | 0.412      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.0328     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 29.2       |\n",
            "|    n_updates            | 840        |\n",
            "|    policy_gradient_loss | 0.0107     |\n",
            "|    reward               | 2.1219847  |\n",
            "|    reward_max           | 34.198097  |\n",
            "|    reward_mean          | 0.12682717 |\n",
            "|    reward_min           | -40.82223  |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 48.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1406        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031757344 |\n",
            "|    clip_fraction        | 0.307       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.0216      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 70          |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | 0.0116      |\n",
            "|    reward               | 0.86024237  |\n",
            "|    reward_max           | 36.949986   |\n",
            "|    reward_mean          | 0.10911255  |\n",
            "|    reward_min           | -60.68583   |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 145         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 1423        |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040734157 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | -0.0103     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 153         |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.00311    |\n",
            "|    reward               | 0.31267384  |\n",
            "|    reward_max           | 23.714266   |\n",
            "|    reward_mean          | 0.09649343  |\n",
            "|    reward_min           | -22.773592  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 381         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1444        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035216674 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.0661      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35          |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.00851    |\n",
            "|    reward               | 0.7682004   |\n",
            "|    reward_max           | 35.209606   |\n",
            "|    reward_mean          | 0.13228822  |\n",
            "|    reward_min           | -49.26343   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 58.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 89          |\n",
            "|    time_elapsed         | 1460        |\n",
            "|    total_timesteps      | 182272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030949982 |\n",
            "|    clip_fraction        | 0.332       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.0297      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 246         |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | 0.00182     |\n",
            "|    reward               | -5.2804193  |\n",
            "|    reward_max           | 22.71433    |\n",
            "|    reward_mean          | 0.1810886   |\n",
            "|    reward_min           | -24.085379  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 225         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 90          |\n",
            "|    time_elapsed         | 1476        |\n",
            "|    total_timesteps      | 184320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041505113 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.112       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.9        |\n",
            "|    n_updates            | 890         |\n",
            "|    policy_gradient_loss | 0.00281     |\n",
            "|    reward               | -1.2103728  |\n",
            "|    reward_max           | 36.533386   |\n",
            "|    reward_mean          | 0.050916802 |\n",
            "|    reward_min           | -53.891556  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 64.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 1492        |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020500194 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.033       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 146         |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | 0.00242     |\n",
            "|    reward               | 7.6209908   |\n",
            "|    reward_max           | 39.386585   |\n",
            "|    reward_mean          | 0.17808418  |\n",
            "|    reward_min           | -65.50491   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 218         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 1508        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.058805324 |\n",
            "|    clip_fraction        | 0.402       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.0113      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 209         |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | 0.00968     |\n",
            "|    reward               | -10.873854  |\n",
            "|    reward_max           | 23.202951   |\n",
            "|    reward_mean          | 0.03498777  |\n",
            "|    reward_min           | -26.083944  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 371         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 1524        |\n",
            "|    total_timesteps      | 190464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041249838 |\n",
            "|    clip_fraction        | 0.348       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | -0.011      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 60.2        |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | 0.00302     |\n",
            "|    reward               | -0.47638112 |\n",
            "|    reward_max           | 40.119354   |\n",
            "|    reward_mean          | 0.07168451  |\n",
            "|    reward_min           | -71.83311   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 114         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 1541        |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029818134 |\n",
            "|    clip_fraction        | 0.299       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.00615     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 285         |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | 0.00346     |\n",
            "|    reward               | 2.5548844   |\n",
            "|    reward_max           | 30.161346   |\n",
            "|    reward_mean          | 0.21100411  |\n",
            "|    reward_min           | -28.580334  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 527         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 124          |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 1557         |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.027267868  |\n",
            "|    clip_fraction        | 0.229        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -45.1        |\n",
            "|    explained_variance   | 0.0638       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 26.2         |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.0131      |\n",
            "|    reward               | 1.4898739    |\n",
            "|    reward_max           | 39.050194    |\n",
            "|    reward_mean          | -0.029599765 |\n",
            "|    reward_min           | -69.10196    |\n",
            "|    std                  | 1.15         |\n",
            "|    value_loss           | 73.7         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 1572        |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025697812 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | -0.00962    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 189         |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | -0.00202    |\n",
            "|    reward               | -9.642988   |\n",
            "|    reward_max           | 34.418518   |\n",
            "|    reward_mean          | 0.13456646  |\n",
            "|    reward_min           | -51.304626  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 441         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 1589        |\n",
            "|    total_timesteps      | 198656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026273612 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.00513     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 64.6        |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.00272    |\n",
            "|    reward               | -3.64551    |\n",
            "|    reward_max           | 14.626208   |\n",
            "|    reward_mean          | 0.060626697 |\n",
            "|    reward_min           | -17.732283  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 157         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 1605        |\n",
            "|    total_timesteps      | 200704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039171398 |\n",
            "|    clip_fraction        | 0.307       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.0909      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.3        |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | -0.00383    |\n",
            "|    reward               | 1.0645865   |\n",
            "|    reward_max           | 39.113472   |\n",
            "|    reward_mean          | 0.16250592  |\n",
            "|    reward_min           | -45.08793   |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 64.8        |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/PPO_5k_4\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JSAHhV4Xc-bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001, 'device': 'cuda'}\n",
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [],
      "source": [
        "trained_td3 = model_td3.learn(total_timesteps=50000, tb_log_name='td3')if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td32\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xwOhVjqRkCdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1', 'device': 'cuda'}\n",
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [],
      "source": [
        "trained_sac = model_sac.learn(total_timesteps=70000, tb_log_name='sac')if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac2\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
